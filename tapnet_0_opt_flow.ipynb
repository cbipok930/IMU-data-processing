{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe8bb76-77d2-4c0c-8492-f84c0451f288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get update && apt-get install wget -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a559755",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!apt-get update && apt-get install libopenexr-dev -y\n",
    "# !python3 -m pip install mediapy einshape chex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a9913c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/deepmind/tapnet.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0abc0d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# !cd tapnet/tapnet && pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92a2c5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export PYTHONPATH=`(cd ./ && pwd)`:`pwd`:$PYTHONPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735696ee-5f3e-4d09-8e4a-f64e3b378a93",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d63906d3-e4ab-44ce-8465-f8f332ddccf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start polling frames\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process\n",
    "from multiprocessing import Event as PEvent\n",
    "from multiprocessing import Queue as PQueue\n",
    "import cv2 \n",
    "from time import sleep, time\n",
    "class Camera():\n",
    "\n",
    "    def __init__(self, cap_id):\n",
    "        self._cap = cv2.VideoCapture(cap_id)\n",
    "        assert self._cap.isOpened(), f'can not connect to camera with id {cap_id}'\n",
    "        \n",
    "\n",
    "    def run(self, stop_event, outque):\n",
    "        time_out = time()\n",
    "        print('start polling frames')\n",
    "        while not stop_event.is_set():\n",
    "            try:\n",
    "                ret, frame = self._cap.read()\n",
    "                outque.put([frame, time()])\n",
    "                #print(time() - time_out, end = '        \\r')\n",
    "                if outque.full():\n",
    "                    frame_out, time_out = outque.get()\n",
    "            except KeyboardInterrupt:\n",
    "                pass\n",
    "             \n",
    "        self._cap.release()\n",
    "        print()\n",
    "        print(f'frames polling stopped               {not self._cap.isOpened()}',)\n",
    "vidstop = PEvent()\n",
    "vidque = PQueue(2)\n",
    "vcap = Camera(0)\n",
    "video_poll_process = Process(target=vcap.run, args=(vidstop, vidque, ))\n",
    "video_poll_process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5461c41-0449-458e-80a1-fb07dcf30d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start polling imu values\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt, atan2, pi\n",
    "import math\n",
    "from adafruit_bno08x.i2c import BNO08X_I2C\n",
    "from adafruit_bno08x import BNO_REPORT_LINEAR_ACCELERATION, BNO_REPORT_ACCELEROMETER, BNO_REPORT_GYROSCOPE, BNO_REPORT_MAGNETOMETER, BNO_REPORT_GRAVITY, BNO_REPORT_ROTATION_VECTOR\n",
    "import busio\n",
    "def find_roll(dqw, dqx, dqy, dqz):\n",
    "    norm = sqrt(dqw * dqw + dqx * dqx + dqy * dqy + dqz * dqz)\n",
    "    dqw = dqw / norm\n",
    "    dqx = dqx / norm\n",
    "    dqy = dqy / norm\n",
    "    dqz = dqz / norm\n",
    "    sinr_cosp = +2.0 * (dqw * dqx + dqy * dqz)\n",
    "    cosr_cosp = +1.0 - 2.0 * (dqx * dqx + dqy * dqy)\n",
    "    roll_raw = atan2(sinr_cosp, cosr_cosp)\n",
    "    roll = roll_raw * 180.0 / pi\n",
    "    if roll > 0:\n",
    "        roll = 360 - roll\n",
    "    else:\n",
    "        roll = abs(roll)\n",
    "    return roll\n",
    "\n",
    "def find_pitch(dqw, dqx, dqy, dqz):\n",
    "    norm = sqrt(dqw * dqw + dqx * dqx + dqy * dqy + dqz * dqz)\n",
    "    dqw = dqw / norm\n",
    "    dqx = dqx / norm\n",
    "    dqy = dqy / norm\n",
    "    dqz = dqz / norm\n",
    "    sinp = sqrt(+1.0 + 2.0 * (dqw * dqy - dqx * dqz))\n",
    "    cosp = sqrt(+1.0 - 2.0 * (dqw * dqy - dqx * dqz))\n",
    "    pitch_raw = +2.0 * atan2(sinp, cosp) - (pi / 2.0)\n",
    "    pitch = pitch_raw * 180.0 / pi\n",
    "    if pitch > 0:\n",
    "        pitch = 360 - pitch\n",
    "    else:\n",
    "        pitch = abs(pitch)\n",
    "    return pitch\n",
    "\n",
    "i2c = busio.I2C((1, 14), (1, 15))\n",
    "imu_device = BNO08X_I2C(i2c, address=0x4b)\n",
    "for feature in [BNO_REPORT_ACCELEROMETER, BNO_REPORT_GYROSCOPE, BNO_REPORT_MAGNETOMETER, BNO_REPORT_ROTATION_VECTOR]:\n",
    "    imu_device.enable_feature(feature)\n",
    "    \n",
    "imu_stop = PEvent()\n",
    "imu_que = PQueue(2)\n",
    "\n",
    "def run_imu(imu_device, ev, que):\n",
    "    time_out = time()\n",
    "    print('start polling imu values')\n",
    "    while not ev.is_set():\n",
    "        try:\n",
    "            q = imu_device.quaternion\n",
    "            que.put([q, time()])\n",
    "            if que.full():\n",
    "                q, time_out = que.get()\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "    print(\"imu polling stopped\")\n",
    "\n",
    "imu_poll_process = Process(target=run_imu, args=(imu_device, imu_stop, imu_que,))\n",
    "imu_poll_process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d900abb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mediapy as media\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from tapnet.torch import tapir_model\n",
    "from tapnet.utils import transforms\n",
    "from tapnet.utils import viz_utils\n",
    "\n",
    "import cv2 \n",
    "import numpy as np\n",
    "from time import sleep, time\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def postprocess_occlusions(occlusions, expected_dist):\n",
    "  visibles = (1 - F.sigmoid(occlusions)) * (1 - F.sigmoid(expected_dist)) > 0.5\n",
    "  return visibles\n",
    "\n",
    "def preprocess_frames(frames):\n",
    "  \"\"\"Preprocess frames to model inputs.\n",
    "\n",
    "  Args:\n",
    "    frames: [num_frames, height, width, 3], [0, 255], np.uint8\n",
    "\n",
    "  Returns:\n",
    "    frames: [num_frames, height, width, 3], [-1, 1], np.float32\n",
    "  \"\"\"\n",
    "  frames = frames.float()\n",
    "  frames = frames / 255 * 2 - 1\n",
    "  return frames\n",
    "\n",
    "\n",
    "def transform(frame):\n",
    "    frame =  cv2.resize(frame, (fsize , fsize))\n",
    "    tframe = torch.tensor(frame).permute(2,0,1)\n",
    "    crop = tframe.float().permute(1,2,0)\n",
    "    # crop = T.CenterCrop(fsize)(tframe).float().permute(1,2,0)\n",
    "    return crop / 255 * 2 - 1\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model = tapir_model.TAPIR(pyramid_level=1, initial_resolution = (64, 64))\n",
    "\n",
    "model.load_state_dict(torch.load('tapnet/bootstapir_checkpoint.pt'))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b84c4d-b4f4-4fa7-97d4-0e9c5f39164a",
   "metadata": {},
   "source": [
    "# SIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2413da22-16ee-4809-b257-d541e80a5271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2  as cv\n",
    "\n",
    "CaM = np.array([[628.07730444,   0.        , 296.22347846],\n",
    "       [  0.        , 624.87736912, 280.1399009 ],\n",
    "       [  0.        ,   0.        ,   1.        ]])\n",
    "\n",
    "\n",
    "tcrop = None\n",
    "from IPython.display import display as IPdisplay, Image as IPImage\n",
    "\n",
    "display_handle=IPdisplay(None, display_id=True)\n",
    "tdpp = None\n",
    "\n",
    "dpp_new = np.array([0, 0])\n",
    "# first_frame_flag = True\n",
    "try:\n",
    "    while True:\n",
    "        tic = time()\n",
    "        cnt+=1\n",
    "        frame, _ = vidque.get()\n",
    "        q, tic_r = imu_que.get()\n",
    "        \n",
    "        tan_x = np.tan(-math.radians(find_pitch(q[3], q[0], q[1], q[2])))\n",
    "        tan_y = np.tan(-math.radians(find_roll(q[3], q[0], q[1], q[2])))\n",
    "        dpp = np.asarray([\n",
    "            tan_x*CaM[0,0] + CaM[0,2],\n",
    "            tan_y*CaM[1,1] + CaM[1,2],\n",
    "        ])\n",
    "\n",
    "        cv2.circle(frame, dpp.astype(np.int32).tolist(), 8, (255, 0, 0), -1)\n",
    "        crop = frame        \n",
    "        \n",
    "        if tcrop is None:\n",
    "            tframe = frame\n",
    "            tcrop = crop\n",
    "        if tdpp is None:\n",
    "            tdpp = dpp\n",
    "            tdpp = np.array([tdpp])\n",
    "\n",
    "        color_point = (0, 255, 0)\n",
    "\n",
    "        # Initiate SIFT detector\n",
    "        sift = cv2.SIFT_create() \n",
    "         \n",
    "        # find the keypoints and descriptors with SIFT\n",
    "        kp1, des1 = sift.detectAndCompute(cv2.cvtColor(tframe, cv2.COLOR_BGR2GRAY),None)\n",
    "        kp2, des2 = sift.detectAndCompute(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY),None)\n",
    "         \n",
    "        flann = cv.FlannBasedMatcher(dict(algorithm = 0, trees = 5), dict())\n",
    "\n",
    "        if(len(kp1)>=2 and len(kp2)>=2):\n",
    "            matches = flann.knnMatch(des1,des2,k=2)\n",
    "        else:\n",
    "            color_point = (255, 0, 0)\n",
    "            matches = None\n",
    "         \n",
    "        # store all the good matches as per Lowe's ratio test.\n",
    "        good = []\n",
    "        if matches is not None:\n",
    "            for m,n in matches:\n",
    "                if m.distance < 0.7*n.distance:\n",
    "                    good.append(m)\n",
    "            src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "            dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "        else:\n",
    "            dst_pts = []\n",
    "\n",
    "        if (len(dst_pts) >=4):\n",
    "            matrix = cv.findHomography(src_pts, dst_pts, cv.RANSAC,5.0)\n",
    "        else:\n",
    "            matrix = None\n",
    "        \n",
    "        # print(matrix[0])\n",
    "        \n",
    "        # matrix = cv2.findHomography(tracks[:,0,:].numpy().reshape(-1,1,2), tracks[:,1,:].numpy().reshape(-1,1,2), cv2.RANSAC, 5.0)[0]\n",
    "\n",
    "\n",
    "        # np.linalg.inv(matrix)\n",
    "        if matrix is not None and matrix[0] is not None:\n",
    "            dpp_new = cv2.perspectiveTransform(tdpp.reshape(-1,1,2).astype(np.float32), matrix[0])\n",
    "            dpp_new = dpp_new[0][0]\n",
    "        else:\n",
    "            color_point = (255, 0, 0)\n",
    "            dpp_new[0]-=640\n",
    "        shifts = dpp_new - dpp\n",
    " \n",
    "    \n",
    "        crops = []\n",
    "        for img in [tcrop, crop]:\n",
    "            # img = ((img.numpy() + 1) * 255 / 2).astype(np.uint8)\n",
    "            crops.append(img)\n",
    "    \n",
    "\n",
    "\n",
    "        for index_, crop in enumerate(crops):\n",
    "            crops[index_] = crop\n",
    "        canvas = np.hstack(crops)\n",
    "        \n",
    "\n",
    "        frame = np.ndarray.astype(canvas[...,::-1], np.float32)\n",
    "        frame = cv2.putText(frame, f\"{1/(time()-tic):.2f}\", (50, 50) , cv2.FONT_HERSHEY_SIMPLEX ,  \n",
    "                           1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        frame = cv2.putText(frame, f\"{shifts[0]:.2f},{shifts[1]:.2f}\", (50, 200) , cv2.FONT_HERSHEY_SIMPLEX ,  \n",
    "                           1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        dpp_new[0]+=640\n",
    "        # print(dpp_new)\n",
    "        cv2.circle(frame, dpp_new.astype(np.int32).tolist(), 8, color_point, -1)\n",
    "        \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        _, frame = cv2.imencode('.jpeg', frame)\n",
    "        display_handle.update(IPImage(data=frame.tobytes()))\n",
    "        \n",
    "        # print(f'{1/(time()-tic):.2f}', end='          \\r')\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    display_handle.update(None)\n",
    "#Image.fromarray(canvas[...,::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe1a36e-d4a0-43f9-8bdd-36e937f6d277",
   "metadata": {},
   "source": [
    "# TAPIR one point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "184a1726-736d-48a5-8bc8-14b5e1b5fd81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.43046142  8.13807492]\n",
      "[34.30026961 14.60707821]\n",
      "[21.74726088 56.3335962 ]\n",
      "[ -7.68830708 106.0557507 ]\n",
      "[ 6.03629487 34.50278435]\n",
      "[-43.00403011  69.15985251]\n",
      "[ 6.79189008 15.27921557]\n",
      "[-22.55337617  64.60778836]\n",
      "[-45.91546014 112.26545663]\n",
      "[-39.48814148  36.13306947]\n",
      "[-8.38055149 -9.62078896]\n",
      "[ 46.26659857 -25.82812456]\n",
      "[  86.40590676 -111.31186312]\n",
      "[ 88.59746652 -46.73912761]\n",
      "[113.5342484  -54.86136432]\n"
     ]
    }
   ],
   "source": [
    "fsize = 160\n",
    "step = 8\n",
    "\n",
    "CaM = np.array([[628.07730444,   0.        , 296.22347846],\n",
    "       [  0.        , 624.87736912, 280.1399009 ],\n",
    "       [  0.        ,   0.        ,   1.        ]])\n",
    "\n",
    "points = np.asarray([[0, i+step//2, j+step//2] for i in range(0, fsize, step) for j in range(0, fsize, step)])\n",
    "points = torch.tensor(points)[None].float().to(device)\n",
    "\n",
    "tcrop = None\n",
    "from IPython.display import display as IPdisplay, Image as IPImage\n",
    "\n",
    "display_handle=IPdisplay(None, display_id=True)\n",
    "tdpp = None\n",
    "\n",
    "cnt = 0\n",
    "try:\n",
    "    while True:\n",
    "        tic = time()\n",
    "        cnt+=1\n",
    "        frame, _ = vidque.get()\n",
    "        q, tic_r = imu_que.get()\n",
    "        \n",
    "        tan_x = np.tan(-math.radians(find_pitch(q[3], q[0], q[1], q[2])))\n",
    "        tan_y = np.tan(-math.radians(find_roll(q[3], q[0], q[1], q[2])))\n",
    "        dpp = np.asarray([\n",
    "            tan_x*CaM[0,0] + CaM[0,2],\n",
    "            tan_y*CaM[1,1] + CaM[1,2],\n",
    "        ])\n",
    "\n",
    "        cv2.circle(frame, dpp.astype(np.int32).tolist(), 8, (255, 0, 0), -1)\n",
    "        crop = transform(frame)\n",
    "        dpp[0] *= (fsize/640)\n",
    "        dpp[1] *= (fsize/480)\n",
    " \n",
    "              \n",
    "        \n",
    "        if tcrop is None:\n",
    "            tframe = frame\n",
    "            tcrop = crop\n",
    "        if tdpp is None:\n",
    "            tdpp = dpp\n",
    "            points = [[0., tdpp[1], tdpp[0]]]\n",
    "            points = torch.tensor(points)[None].float().to(device)\n",
    "            tdpp = np.array([tdpp])\n",
    "\n",
    "\n",
    "        seq = torch.cat((tcrop[None], crop[None]))[None]\n",
    "    \n",
    "        outputs = model(seq, points)\n",
    "        tracks, occlusions, expected_dist = outputs['tracks'][0], outputs['occlusion'][0], outputs['expected_dist'][0]\n",
    "        # print((tracks[0][1]).numpy())\n",
    "        # matrix = cv2.findHomography(tracks[:,0,:].numpy().reshape(-1,1,2), tracks[:,1,:].numpy().reshape(-1,1,2), cv2.RANSAC, 5.0)[0]\n",
    "\n",
    "        # dpp_new = cv2.perspectiveTransform(tdpp.reshape(-1,1,2).astype(np.float32), matrix)\n",
    "\n",
    "                \n",
    "        dpp_new = (tracks[0][1]).numpy()\n",
    "        # dpp_new = dpp_new[0][0]\n",
    "        shifts = dpp_new - dpp\n",
    "        shifts[0]*=(640/fsize)\n",
    "        shifts[1]*=(480/fsize)\n",
    "        print(shifts)\n",
    "\n",
    "        crops = []\n",
    "        for img in [tcrop, crop]:\n",
    "            img = ((img.numpy() + 1) * 255 / 2).astype(np.uint8)\n",
    "            crops.append(img)\n",
    "        for pts in tracks:\n",
    "            color = np.random.randint(low=0, high=255,size=3).tolist()\n",
    "            for pt, img in zip(pts, crops):\n",
    "                cv2.circle(img, pt.numpy().astype(np.int32).tolist(), 1, color, -1)\n",
    "\n",
    "        for index_, crop in enumerate(crops):\n",
    "            crop = cv2.resize(crop, (640, 480))\n",
    "            crops[index_] = crop\n",
    "        canvas = np.hstack(crops)\n",
    "        \n",
    "\n",
    "        frame = np.ndarray.astype(canvas[...,::-1], np.float32)\n",
    "        frame = cv2.putText(frame, f\"{1/(time()-tic):.2f}\", (50, 50) , cv2.FONT_HERSHEY_SIMPLEX ,  \n",
    "                           1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        dpp_new[0]*=(640/fsize)\n",
    "        dpp_new[1]*=(480/fsize)\n",
    "        dpp_new[0]+=640\n",
    "        cv2.circle(frame, dpp_new.astype(np.int32).tolist(), 8, (0, 255, 0), -1)\n",
    "        \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        _, frame = cv2.imencode('.jpeg', frame)\n",
    "        display_handle.update(IPImage(data=frame.tobytes()))\n",
    "        \n",
    "        if cnt > 19:\n",
    "            print(\"OVER\")\n",
    "        if cnt > 20:\n",
    "            cnt = 0\n",
    "            clear_output(wait=True)\n",
    "            display_handle=IPdisplay(None, display_id=True)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    display_handle.update(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1f9795-53f0-423a-b0e5-ed44a1d7f038",
   "metadata": {},
   "source": [
    "# TAPIR homography\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7118a6bc-3ae1-4426-82cf-12d9d5376a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 32.31164606 -42.05538609]\n",
      "[  8.61859008 114.7506734 ]\n",
      "[ 76.94139724 -29.16655895]\n",
      "[ 60.06923469 -23.10683761]\n",
      "[89.94264626 -2.10244784]\n",
      "[63.95981521 57.29020157]\n",
      "[99.70249814 30.0022021 ]\n",
      "[63.06241207  6.38688062]\n",
      "[76.04340493 53.93290018]\n",
      "[62.49003813 52.18867038]\n",
      "[69.85384771 44.07788093]\n",
      "[27.44661738 53.97820926]\n",
      "[84.79117535  1.40461785]\n",
      "[54.12595043 68.9701967 ]\n",
      "[91.68241022 40.45695344]\n",
      "[91.05444998 29.7395854 ]\n",
      "[100.95083291  -2.24109254]\n",
      "[82.82016772 51.98131021]\n",
      "[51.81215457 50.36100391]\n",
      "[16.27251171 61.07455409]\n",
      "OVER\n"
     ]
    }
   ],
   "source": [
    "fsize = 64#256\n",
    "step = 8#64*4\n",
    "\n",
    "CaM = np.array([[628.07730444,   0.        , 296.22347846],\n",
    "       [  0.        , 624.87736912, 280.1399009 ],\n",
    "       [  0.        ,   0.        ,   1.        ]])\n",
    "\n",
    "points = np.asarray([[0, i+step//2, j+step//2] for i in range(0, fsize, step) for j in range(0, fsize, step)])\n",
    "points = torch.tensor(points)[None].float().to(device)\n",
    "\n",
    "tcrop = None\n",
    "from IPython.display import display as IPdisplay, Image as IPImage\n",
    "\n",
    "display_handle=IPdisplay(None, display_id=True)\n",
    "tdpp = None\n",
    "\n",
    "cnt = 0\n",
    "# first_frame_flag = True\n",
    "try:\n",
    "    while True:\n",
    "        tic = time()\n",
    "        cnt+=1\n",
    "        # ret, frame = cap.read()\n",
    "        frame, _ = vidque.get()\n",
    "        q, tic_r = imu_que.get()\n",
    "        \n",
    "        tan_x = np.tan(-math.radians(find_pitch(q[3], q[0], q[1], q[2])))\n",
    "        tan_y = np.tan(-math.radians(find_roll(q[3], q[0], q[1], q[2])))\n",
    "        dpp = np.asarray([\n",
    "            tan_x*CaM[0,0] + CaM[0,2],\n",
    "            tan_y*CaM[1,1] + CaM[1,2],\n",
    "        ])\n",
    "\n",
    "        cv2.circle(frame, dpp.astype(np.int32).tolist(), 8, (255, 0, 0), -1)\n",
    "        crop = transform(frame)\n",
    "        dpp[0] *= (fsize/640)\n",
    "        dpp[1] *= (fsize/480)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if tcrop is None:\n",
    "            tframe = frame\n",
    "            tcrop = crop\n",
    "        if tdpp is None:\n",
    "            tdpp = dpp\n",
    "            tdpp = np.array([tdpp])\n",
    "    \n",
    "        seq = torch.cat((tcrop[None], crop[None]))[None]\n",
    "    \n",
    "        outputs = model(seq, points)\n",
    "        tracks, occlusions, expected_dist = outputs['tracks'][0], outputs['occlusion'][0], outputs['expected_dist'][0]\n",
    "        matrix = cv2.findHomography(tracks[:,0,:].numpy().reshape(-1,1,2), tracks[:,1,:].numpy().reshape(-1,1,2), cv2.RANSAC, 5.0)[0]\n",
    "\n",
    "\n",
    "        # np.linalg.inv(matrix)\n",
    "        dpp_new = cv2.perspectiveTransform(tdpp.reshape(-1,1,2).astype(np.float32), matrix)\n",
    "        dpp_new = dpp_new[0][0]\n",
    "        shifts = dpp_new - dpp\n",
    "        shifts[0]*=(640/fsize)\n",
    "        shifts[1]*=(480/fsize)\n",
    "        print(shifts)\n",
    "        # dpp[0] /= (fsize/480)\n",
    "        # dpp[1] /= (fsize/640)\n",
    "        \n",
    "        # crop = transform(frame)\n",
    "        \n",
    "        \n",
    "        # print(dpp, np.dot(np.array(np.append(dpp, 0)), matrix))\n",
    "        # dpp = np.dot(matrix, np.array(np.append(dpp, 0)))\n",
    "\n",
    "\n",
    "        # rotated = np.dot(np.linalg.inv(matrix), np.array(np.append(dpp, 0)))\n",
    "        # print(rotated[:2] - tdpp)\n",
    " \n",
    "    \n",
    "        crops = []\n",
    "        for img in [tcrop, crop]:\n",
    "            img = ((img.numpy() + 1) * 255 / 2).astype(np.uint8)\n",
    "            crops.append(img)\n",
    "    \n",
    "    \n",
    "        # for pts in tracks:\n",
    "        #     color = np.random.randint(low=0, high=255,size=3).tolist()\n",
    "        #     for pt, img in zip(pts, crops):\n",
    "        #         cv2.circle(img, pt.numpy().astype(np.int32).tolist(), 1, color, -1)\n",
    "        \"\"\"\n",
    "        tcrop = crop\n",
    "        \n",
    "        tframe = frame\n",
    "\n",
    "        tdpp = dpp\n",
    "        \"\"\"\n",
    "        #viz\n",
    "\n",
    "        for index_, crop in enumerate(crops):\n",
    "            crop = cv2.resize(crop, (640, 480))\n",
    "            crops[index_] = crop\n",
    "        canvas = np.hstack(crops)\n",
    "        \n",
    "\n",
    "        frame = np.ndarray.astype(canvas[...,::-1], np.float32)\n",
    "        frame = cv2.putText(frame, f\"{1/(time()-tic):.2f}\", (50, 50) , cv2.FONT_HERSHEY_SIMPLEX ,  \n",
    "                           1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        dpp_new[0]*=(640/fsize)\n",
    "        dpp_new[1]*=(480/fsize)\n",
    "        dpp_new[0]+=640\n",
    "        # print(dpp_new)\n",
    "        cv2.circle(frame, dpp_new.astype(np.int32).tolist(), 8, (0, 255, 0), -1)\n",
    "        \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        _, frame = cv2.imencode('.jpeg', frame)\n",
    "        display_handle.update(IPImage(data=frame.tobytes()))\n",
    "        \n",
    "        # print(f'{1/(time()-tic):.2f}', end='          \\r')\n",
    "        if cnt > 19:\n",
    "            print(\"OVER\")\n",
    "        if cnt > 20:\n",
    "            cnt = 0\n",
    "            clear_output(wait=True)\n",
    "            display_handle=IPdisplay(None, display_id=True)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    display_handle.update(None)\n",
    "#Image.fromarray(canvas[...,::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2544dc3a-d5a8-4e6e-a962-01be12bb47b7",
   "metadata": {},
   "source": [
    "# Two adjacent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b49a8440-5a31-4a21-808b-8475e4641b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0598056  -0.03940021]\n",
      "[0.64811145 0.71492192]\n",
      "[-0.5488951  -0.20112309]\n",
      "[ 3.38596381 -1.17410553]\n",
      "[ 13.7591995  -10.27848719]\n",
      "[ 8.14314864 -4.43207242]\n",
      "[1.99610605 2.69981625]\n",
      "[ 2.2273344  -0.81749945]\n",
      "[-2.82252744  4.83432838]\n",
      "[-2.60741308  2.13760683]\n",
      "[-1.79434008  4.69256638]\n",
      "[0.78470077 8.56662227]\n",
      "[-5.05555492  2.80911642]\n",
      "[ 1.72981707 -2.13362267]\n",
      "[3.1146057  0.41752729]\n",
      "[-1.60937748 -5.02864305]\n",
      "[-1.49917126 -0.78063396]\n"
     ]
    }
   ],
   "source": [
    "fsize = 64#256\n",
    "step = 8#64*4\n",
    "\n",
    "CaM = np.array([[628.07730444,   0.        , 296.22347846],\n",
    "       [  0.        , 624.87736912, 280.1399009 ],\n",
    "       [  0.        ,   0.        ,   1.        ]])\n",
    "\n",
    "points = np.asarray([[0, i+step//2, j+step//2] for i in range(0, fsize, step) for j in range(0, fsize, step)])\n",
    "points = torch.tensor(points)[None].float().to(device)\n",
    "\n",
    "tcrop = None\n",
    "from IPython.display import display as IPdisplay, Image as IPImage\n",
    "\n",
    "display_handle=IPdisplay(None, display_id=True)\n",
    "tdpp = None\n",
    "\n",
    "cnt = 0\n",
    "try:\n",
    "    while True:\n",
    "        tic = time()\n",
    "        cnt+=1\n",
    "        # ret, frame = cap.read()\n",
    "        frame, _ = vidque.get()\n",
    "        q, tic_r = imu_que.get()\n",
    "        \n",
    "        tan_x = np.tan(-math.radians(find_pitch(q[3], q[0], q[1], q[2])))\n",
    "        tan_y = np.tan(-math.radians(find_roll(q[3], q[0], q[1], q[2])))\n",
    "        dpp = np.asarray([\n",
    "            tan_x*CaM[0,0] + CaM[0,2],\n",
    "            tan_y*CaM[1,1] + CaM[1,2],\n",
    "        ])\n",
    "\n",
    "        cv2.circle(frame, dpp.astype(np.int32).tolist(), 8, (255, 0, 0), -1)\n",
    "        crop = transform(frame)\n",
    "        dpp[0] *= (fsize/640)\n",
    "        dpp[1] *= (fsize/480)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if tcrop is None:\n",
    "            tframe = frame\n",
    "            tcrop = crop\n",
    "        if tdpp is None:\n",
    "            tdpp = dpp\n",
    "    \n",
    "        seq = torch.cat((tcrop[None], crop[None]))[None]\n",
    "    \n",
    "        outputs = model(seq, points)\n",
    "        tracks, occlusions, expected_dist = outputs['tracks'][0], outputs['occlusion'][0], outputs['expected_dist'][0]\n",
    "        matrix = cv2.findHomography(tracks[:,0,:].numpy().reshape(-1,1,2), tracks[:,1,:].numpy().reshape(-1,1,2), cv2.RANSAC, 5.0)[0]\n",
    "\n",
    "        # print(tdpp)\n",
    "        tdpp = np.array([tdpp])\n",
    "        # np.linalg.inv(matrix)\n",
    "        dpp_new = cv2.perspectiveTransform(tdpp.reshape(-1,1,2).astype(np.float32), matrix)\n",
    "        dpp_new = dpp_new[0][0]\n",
    "        print(dpp_new - dpp)\n",
    "        # dpp[0] /= (fsize/480)\n",
    "        # dpp[1] /= (fsize/640)\n",
    "        \n",
    "        # crop = transform(frame)\n",
    "        \n",
    "        \n",
    "        # print(dpp, np.dot(np.array(np.append(dpp, 0)), matrix))\n",
    "        # dpp = np.dot(matrix, np.array(np.append(dpp, 0)))\n",
    "\n",
    "\n",
    "        # rotated = np.dot(np.linalg.inv(matrix), np.array(np.append(dpp, 0)))\n",
    "        # print(rotated[:2] - tdpp)\n",
    " \n",
    "    \n",
    "        crops = []\n",
    "        for img in [tcrop, crop]:\n",
    "            img = ((img.numpy() + 1) * 255 / 2).astype(np.uint8)\n",
    "            crops.append(img)\n",
    "    \n",
    "    \n",
    "        # for pts in tracks:\n",
    "        #     color = np.random.randint(low=0, high=255,size=3).tolist()\n",
    "        #     for pt, img in zip(pts, crops):\n",
    "        #         cv2.circle(img, pt.numpy().astype(np.int32).tolist(), 1, color, -1)\n",
    "    \n",
    "        tcrop = crop\n",
    "        \n",
    "        tframe = frame\n",
    "\n",
    "        tdpp = dpp\n",
    "\n",
    "        #viz\n",
    "\n",
    "        for index_, crop in enumerate(crops):\n",
    "            crop = cv2.resize(crop, (640, 480))\n",
    "            crops[index_] = crop\n",
    "        canvas = np.hstack(crops)\n",
    "        \n",
    "\n",
    "        frame = np.ndarray.astype(canvas[...,::-1], np.float32)\n",
    "        # frame = cv2.putText(frame, f\"{1/(time()-tic):.2f}\", (50, 50) , cv2.FONT_HERSHEY_SIMPLEX ,  \n",
    "        #                    1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # dpp_new*=(64/fsize)\n",
    "        dpp_new[0]*=(640/fsize)\n",
    "        dpp_new[1]*=(480/fsize)\n",
    "        dpp_new[0]+=640\n",
    "        # print(dpp_new)\n",
    "        cv2.circle(frame, dpp_new.astype(np.int32).tolist(), 8, (0, 255, 0), -1)\n",
    "        \n",
    "        # frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        _, frame = cv2.imencode('.jpeg', frame)\n",
    "        display_handle.update(IPImage(data=frame.tobytes()))\n",
    "        \n",
    "        # print(f'{1/(time()-tic):.2f}', end='          \\r')\n",
    "        if cnt > 19:\n",
    "            print(\"OVER\")\n",
    "        if cnt > 20:\n",
    "            cnt = 0\n",
    "            clear_output(wait=True)\n",
    "            display_handle=IPdisplay(None, display_id=True)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    display_handle.update(None)\n",
    "#Image.fromarray(canvas[...,::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f5104c-5f68-4c1d-9d22-50aa109a6999",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fee2d974-ffeb-48a2-9cb1-3875e582702b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.04653590e+00,  5.33360147e-03, -1.05630577e+00],\n",
       "        [-4.62948390e-03,  1.05435557e+00, -1.45384428e+00],\n",
       "        [-1.28266517e-04,  3.30127736e-04,  1.00000000e+00]]),\n",
       " array([[0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]], dtype=uint8))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.findHomography(tracks[:,0,:].numpy().reshape(-1,1,2), tracks[:,1,:].numpy().reshape(-1,1,2), cv2.RANSAC, 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "515c7e26-a989-4ed8-8a7f-39c7c06a6053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967d1e3a-de06-453d-9437-b2d180754233",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7fe41d9-4e1d-4dca-ad46-a48406a5d307",
   "metadata": {},
   "outputs": [],
   "source": [
    "canvas = tframe.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7eb26345-3da0-48f7-9542-4bbb883813ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 64, 64, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = torch.cat((tcrop[None], crop[None]))[None]\n",
    "seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "132ae86e-d311-4a0f-9ff1-5923ae894d1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'frames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m fsize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n\u001b[0;32m----> 2\u001b[0m tframes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39masarray(\u001b[43mframes\u001b[49m))\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      3\u001b[0m tcrops \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mCenterCrop(fsize)(tframes)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m      4\u001b[0m tcrops \u001b[38;5;241m=\u001b[39m preprocess_frames(tcrops)[\u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'frames' is not defined"
     ]
    }
   ],
   "source": [
    "fsize = 256\n",
    "tframes = torch.tensor(np.asarray(frames)).permute(0,3,1,2)\n",
    "tcrops = T.CenterCrop(fsize)(tframes).float()\n",
    "tcrops = preprocess_frames(tcrops)[None].to(device).permute(0,1,3,4,2)\n",
    "\n",
    "step = 32\n",
    "points = np.asarray([[0, i+step//2, j+step//2] for i in range(0, fsize, step) for j in range(0, fsize, step)])\n",
    "points = torch.tensor(points)[None].float().to(device)\n",
    "#points = points.float()[None]\n",
    "tcrops.shape, points.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2928c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "num_frames, height, width = tcrops.shape[1:4]\n",
    "\n",
    "# Model inference\n",
    "tic = time()\n",
    "outputs = model(tcrops, points)\n",
    "print(num_frames/(time()-tic))\n",
    "\n",
    "tracks, occlusions, expected_dist = outputs['tracks'][0], outputs['occlusion'][0], outputs['expected_dist'][0]\n",
    "\n",
    "# Binarize occlusions\n",
    "visibles = postprocess_occlusions(occlusions, expected_dist)\n",
    "\n",
    "tracks = tracks.cpu().numpy()\n",
    "visibles = visibles.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e55799-6bb4-48bf-b36d-2881fa5201ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://storage.googleapis.com/dm-tapnet/tapir_checkpoint_panning.pt\n",
    "#!wget https://storage.googleapis.com/dm-tapnet/bootstapir_checkpoint.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3e925f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2a5fac-43a3-4fa6-9eda-2cd8e5935b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6996e911-406c-4dc6-8350-3f0412e4633f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8d391f-390e-42c6-aa2a-10a456afd3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as t\n",
    "\n",
    "\n",
    "dir(t)\n",
    "#help(t.CenterCrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e134330b-b340-41b8-85c8-2fe25b8bda14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed67a6d8-c7d4-42ce-ae70-273996cc1690",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
